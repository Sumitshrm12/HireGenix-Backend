"""
ðŸŽ¤ VOICE-NATIVE PROCESSING - Real-Time Speech Intelligence
Enables voice-first interviewing with WebRTC streaming, Whisper transcription,
and prosodic analysis for deeper behavioral insights.

Features:
- WebRTC audio stream processing
- Real-time Whisper transcription
- Prosodic analysis (tone, pace, pauses)
- Speech disfluency detection
- Confidence level from voice patterns
- Multi-language support
- Voice emotion recognition
- Speech clarity scoring

Tech Stack:
- WebRTC for audio streaming
- OpenAI Whisper for transcription
- Librosa for audio analysis
- LangGraph for processing pipeline
"""

import os
import json
import asyncio
import io
import wave
import struct
import tempfile
from typing import Dict, Any, List, Optional, Tuple, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import logging
import numpy as np

# Audio processing
try:
    import librosa
    import soundfile as sf
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    logging.warning("âš ï¸ librosa not available. Audio analysis limited.")

# Whisper for transcription
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from langchain_openai import AzureChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SpeechEmotion(str, Enum):
    """Detected speech emotions"""
    CONFIDENT = "confident"
    NERVOUS = "nervous"
    ENTHUSIASTIC = "enthusiastic"
    HESITANT = "hesitant"
    RELAXED = "relaxed"
    STRESSED = "stressed"
    ENGAGED = "engaged"
    DISENGAGED = "disengaged"


class SpeechClarity(str, Enum):
    """Speech clarity levels"""
    EXCELLENT = "excellent"
    GOOD = "good"
    MODERATE = "moderate"
    POOR = "poor"


class VoiceProcessingState(BaseModel):
    """State for voice processing pipeline"""
    # Audio input
    audio_data: bytes = b""
    sample_rate: int = 16000
    audio_duration: float = 0.0
    
    # Transcription
    transcript: str = ""
    transcript_segments: List[Dict[str, Any]] = Field(default_factory=list)
    word_timestamps: List[Dict[str, Any]] = Field(default_factory=list)
    confidence: float = 0.0
    language: str = "en"
    
    # Prosodic analysis
    speaking_rate: float = 0.0  # words per minute
    pause_patterns: List[Dict[str, Any]] = Field(default_factory=list)
    pitch_variation: float = 0.0
    volume_stability: float = 0.0
    
    # Disfluency analysis
    filler_words: List[Dict[str, Any]] = Field(default_factory=list)
    hesitations: List[Dict[str, Any]] = Field(default_factory=list)
    self_corrections: int = 0
    incomplete_thoughts: int = 0
    
    # Emotion detection
    detected_emotion: str = SpeechEmotion.RELAXED.value
    emotion_confidence: float = 0.0
    emotion_trajectory: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Quality metrics
    speech_clarity: str = SpeechClarity.GOOD.value
    noise_level: float = 0.0
    audio_quality_score: float = 0.0
    
    # Behavioral signals
    confidence_indicators: List[str] = Field(default_factory=list)
    stress_indicators: List[str] = Field(default_factory=list)
    engagement_score: float = 0.0
    
    # Processed output
    enhanced_transcript: str = ""
    behavioral_summary: str = ""


class VoiceNativeProcessor:
    """
    Real-time voice processing engine for interview analysis.
    Combines transcription, prosodic analysis, and behavioral insights.
    """
    
    def __init__(self):
        logger.info("ðŸŽ¤ Initializing Voice-Native Processor...")
        
        # Azure OpenAI for Whisper API
        self.openai_client = None
        if OPENAI_AVAILABLE:
            from openai import AzureOpenAI
            self.openai_client = AzureOpenAI(
                api_key=os.getenv("AZURE_OPENAI_API_KEY"),
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview")
            )
        
        # LLM for behavioral analysis
        self.llm = AzureChatOpenAI(
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"),
            temperature=0.3
        )
        
        # Filler word patterns
        self.filler_patterns = [
            "um", "uh", "ah", "er", "like", "you know", "basically",
            "actually", "literally", "sort of", "kind of", "I mean",
            "right", "so", "well", "okay so"
        ]
        
        # Build processing workflow
        self.workflow = self._build_voice_workflow()
        
        # Audio buffer for streaming
        self.audio_buffer = io.BytesIO()
        
        logger.info("âœ… Voice-Native Processor initialized")
    
    def _build_voice_workflow(self) -> StateGraph:
        """Build LangGraph workflow for voice processing"""
        workflow = StateGraph(VoiceProcessingState)
        
        # Define nodes
        workflow.add_node("transcribe_audio", self._transcribe_audio)
        workflow.add_node("analyze_prosody", self._analyze_prosody)
        workflow.add_node("detect_disfluencies", self._detect_disfluencies)
        workflow.add_node("analyze_emotion", self._analyze_emotion)
        workflow.add_node("extract_behavioral_signals", self._extract_behavioral_signals)
        workflow.add_node("generate_summary", self._generate_summary)
        
        # Define edges
        workflow.set_entry_point("transcribe_audio")
        workflow.add_edge("transcribe_audio", "analyze_prosody")
        workflow.add_edge("analyze_prosody", "detect_disfluencies")
        workflow.add_edge("detect_disfluencies", "analyze_emotion")
        workflow.add_edge("analyze_emotion", "extract_behavioral_signals")
        workflow.add_edge("extract_behavioral_signals", "generate_summary")
        workflow.add_edge("generate_summary", END)
        
        return workflow.compile()
    
    async def _transcribe_audio(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Transcribe audio using Whisper"""
        try:
            if not state.audio_data:
                logger.warning("âš ï¸ No audio data to transcribe")
                return state
            
            if not self.openai_client:
                logger.warning("âš ï¸ OpenAI client not available")
                state.transcript = "[Transcription unavailable - client not initialized]"
                return state
            
            # Save audio to temp file for Whisper API
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                temp_path = temp_file.name
                # Write WAV header and data
                with wave.open(temp_file, 'wb') as wav:
                    wav.setnchannels(1)
                    wav.setsampwidth(2)  # 16-bit
                    wav.setframerate(state.sample_rate)
                    wav.writeframes(state.audio_data)
            
            try:
                # Call Whisper API
                with open(temp_path, 'rb') as audio_file:
                    response = self.openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        response_format="verbose_json",
                        timestamp_granularities=["word", "segment"]
                    )
                
                state.transcript = response.text
                state.language = getattr(response, 'language', 'en')
                
                # Extract word timestamps if available
                if hasattr(response, 'words'):
                    state.word_timestamps = [
                        {
                            "word": w.word,
                            "start": w.start,
                            "end": w.end
                        }
                        for w in response.words
                    ]
                
                # Extract segments
                if hasattr(response, 'segments'):
                    state.transcript_segments = [
                        {
                            "text": s.text,
                            "start": s.start,
                            "end": s.end
                        }
                        for s in response.segments
                    ]
                
                # Calculate confidence (average of segment confidences if available)
                state.confidence = 0.85  # Default high confidence for Whisper
                
            finally:
                # Clean up temp file
                import os
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
            
            logger.info(f"ðŸ“ Transcribed {len(state.transcript)} characters")
            return state
            
        except Exception as e:
            logger.error(f"âŒ Transcription error: {e}")
            state.transcript = f"[Transcription error: {str(e)}]"
            return state
    
    async def _analyze_prosody(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Analyze prosodic features of speech"""
        try:
            if not LIBROSA_AVAILABLE or not state.audio_data:
                # Fallback to text-based analysis
                return await self._text_based_prosody_analysis(state)
            
            # Convert bytes to numpy array
            audio_array = np.frombuffer(state.audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            
            if len(audio_array) < state.sample_rate * 0.5:  # Less than 0.5 seconds
                return state
            
            # Calculate duration
            state.audio_duration = len(audio_array) / state.sample_rate
            
            # Calculate speaking rate (words per minute)
            word_count = len(state.transcript.split())
            state.speaking_rate = (word_count / state.audio_duration) * 60 if state.audio_duration > 0 else 0
            
            # Analyze pitch variation using librosa
            try:
                pitches, magnitudes = librosa.piptrack(y=audio_array, sr=state.sample_rate)
                pitch_values = pitches[magnitudes > np.median(magnitudes)]
                pitch_values = pitch_values[pitch_values > 0]
                
                if len(pitch_values) > 0:
                    state.pitch_variation = float(np.std(pitch_values) / np.mean(pitch_values))
            except:
                state.pitch_variation = 0.3  # Default moderate variation
            
            # Analyze volume stability (RMS energy variation)
            try:
                rms = librosa.feature.rms(y=audio_array)[0]
                state.volume_stability = 1 - (np.std(rms) / (np.mean(rms) + 1e-6))
            except:
                state.volume_stability = 0.7
            
            # Detect pauses using silence detection
            try:
                intervals = librosa.effects.split(audio_array, top_db=30)
                pauses = []
                for i in range(len(intervals) - 1):
                    pause_start = intervals[i][1] / state.sample_rate
                    pause_end = intervals[i+1][0] / state.sample_rate
                    pause_duration = pause_end - pause_start
                    
                    if pause_duration > 0.3:  # Significant pause
                        pauses.append({
                            "start": pause_start,
                            "end": pause_end,
                            "duration": pause_duration,
                            "type": "thinking" if pause_duration < 1.5 else "hesitation"
                        })
                
                state.pause_patterns = pauses
            except:
                pass
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Prosody analysis error: {e}")
            return state
    
    async def _text_based_prosody_analysis(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Fallback text-based prosody analysis"""
        if not state.transcript:
            return state
        
        # Estimate speaking rate from word timestamps if available
        if state.word_timestamps and len(state.word_timestamps) > 1:
            total_time = state.word_timestamps[-1].get("end", 1) - state.word_timestamps[0].get("start", 0)
            if total_time > 0:
                state.speaking_rate = (len(state.word_timestamps) / total_time) * 60
        
        # Analyze pauses from timestamps
        if state.word_timestamps:
            pauses = []
            for i in range(len(state.word_timestamps) - 1):
                gap = state.word_timestamps[i+1].get("start", 0) - state.word_timestamps[i].get("end", 0)
                if gap > 0.3:
                    pauses.append({
                        "after_word": state.word_timestamps[i].get("word"),
                        "duration": gap,
                        "type": "thinking" if gap < 1.5 else "hesitation"
                    })
            state.pause_patterns = pauses
        
        return state
    
    async def _detect_disfluencies(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Detect speech disfluencies (filler words, hesitations)"""
        try:
            transcript_lower = state.transcript.lower()
            
            # Detect filler words
            filler_words = []
            for filler in self.filler_patterns:
                count = transcript_lower.count(filler)
                if count > 0:
                    filler_words.append({
                        "word": filler,
                        "count": count,
                        "frequency": count / (len(state.transcript.split()) or 1)
                    })
            
            state.filler_words = sorted(filler_words, key=lambda x: x["count"], reverse=True)
            
            # Detect self-corrections (patterns like "no, I mean", "sorry,")
            correction_patterns = ["no i mean", "sorry", "let me rephrase", "what i meant", "actually no"]
            for pattern in correction_patterns:
                state.self_corrections += transcript_lower.count(pattern)
            
            # Detect incomplete thoughts (sentences that don't end properly)
            sentences = state.transcript.split('.')
            incomplete = sum(1 for s in sentences if s.strip() and s.strip()[-1:] not in '.!?')
            state.incomplete_thoughts = max(0, incomplete - 1)  # Last sentence may be incomplete
            
            # Map hesitation pauses to words
            if state.pause_patterns and state.word_timestamps:
                for pause in state.pause_patterns:
                    if pause.get("type") == "hesitation":
                        state.hesitations.append({
                            "after_word": pause.get("after_word", ""),
                            "duration": pause.get("duration", 0)
                        })
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Disfluency detection error: {e}")
            return state
    
    async def _analyze_emotion(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Analyze emotion from voice characteristics"""
        try:
            # Build feature vector for emotion detection
            features = {
                "speaking_rate": state.speaking_rate,
                "pitch_variation": state.pitch_variation,
                "volume_stability": state.volume_stability,
                "pause_count": len(state.pause_patterns),
                "filler_count": sum(f["count"] for f in state.filler_words),
                "hesitation_count": len(state.hesitations)
            }
            
            # Rule-based emotion detection
            if features["speaking_rate"] > 160 and features["pitch_variation"] > 0.4:
                emotion = SpeechEmotion.ENTHUSIASTIC
                confidence = 0.8
            elif features["filler_count"] > 5 or features["hesitation_count"] > 3:
                emotion = SpeechEmotion.NERVOUS
                confidence = 0.7
            elif features["speaking_rate"] < 100 and features["pause_count"] > 5:
                emotion = SpeechEmotion.HESITANT
                confidence = 0.75
            elif features["volume_stability"] > 0.8 and features["pitch_variation"] > 0.2:
                emotion = SpeechEmotion.CONFIDENT
                confidence = 0.8
            elif features["speaking_rate"] > 120 and features["volume_stability"] > 0.7:
                emotion = SpeechEmotion.ENGAGED
                confidence = 0.75
            else:
                emotion = SpeechEmotion.RELAXED
                confidence = 0.6
            
            state.detected_emotion = emotion.value
            state.emotion_confidence = confidence
            
            state.emotion_trajectory.append({
                "emotion": emotion.value,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            })
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Emotion analysis error: {e}")
            return state
    
    async def _extract_behavioral_signals(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Extract behavioral signals from voice analysis"""
        try:
            # Confidence indicators
            confidence_signals = []
            stress_signals = []
            
            # Speaking rate analysis
            if 120 <= state.speaking_rate <= 160:
                confidence_signals.append("Optimal speaking pace")
            elif state.speaking_rate < 100:
                stress_signals.append("Slow, deliberate speech - may indicate uncertainty")
            elif state.speaking_rate > 180:
                stress_signals.append("Rapid speech - may indicate nervousness")
            
            # Filler word analysis
            total_fillers = sum(f["count"] for f in state.filler_words)
            word_count = len(state.transcript.split())
            filler_ratio = total_fillers / word_count if word_count > 0 else 0
            
            if filler_ratio < 0.02:
                confidence_signals.append("Minimal filler words - articulate")
            elif filler_ratio > 0.08:
                stress_signals.append("High filler word usage - potential uncertainty")
            
            # Pause analysis
            long_pauses = [p for p in state.pause_patterns if p.get("duration", 0) > 2.0]
            if len(long_pauses) == 0:
                confidence_signals.append("Fluid speech without long pauses")
            elif len(long_pauses) > 3:
                stress_signals.append("Multiple long pauses - processing complex thoughts or uncertain")
            
            # Pitch variation
            if 0.2 <= state.pitch_variation <= 0.4:
                confidence_signals.append("Natural pitch variation - engaging")
            elif state.pitch_variation < 0.1:
                stress_signals.append("Monotone delivery - may indicate low engagement")
            
            state.confidence_indicators = confidence_signals
            state.stress_indicators = stress_signals
            
            # Calculate engagement score
            engagement_factors = [
                (state.speaking_rate / 140) if 80 <= state.speaking_rate <= 180 else 0.5,
                state.pitch_variation if 0 < state.pitch_variation < 0.6 else 0.5,
                1 - (filler_ratio * 5),  # Penalize fillers
                state.volume_stability
            ]
            state.engagement_score = sum(engagement_factors) / len(engagement_factors)
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Behavioral signal extraction error: {e}")
            return state
    
    async def _generate_summary(self, state: VoiceProcessingState) -> VoiceProcessingState:
        """Generate behavioral summary from voice analysis"""
        try:
            prompt = f"""
            Generate a brief behavioral summary based on this voice analysis.
            
            TRANSCRIPT:
            {state.transcript[:500]}
            
            VOICE METRICS:
            - Speaking Rate: {state.speaking_rate:.0f} words/minute
            - Pitch Variation: {state.pitch_variation:.2f}
            - Volume Stability: {state.volume_stability:.2f}
            - Detected Emotion: {state.detected_emotion}
            - Filler Words: {[f["word"] for f in state.filler_words[:5]]}
            - Pause Count: {len(state.pause_patterns)}
            
            CONFIDENCE INDICATORS:
            {state.confidence_indicators}
            
            STRESS INDICATORS:
            {state.stress_indicators}
            
            Generate a 2-3 sentence behavioral insight for the interviewer.
            Focus on what these voice patterns suggest about the candidate's state.
            
            Return JSON:
            {{
                "summary": "Brief behavioral insight",
                "speech_clarity": "excellent|good|moderate|poor",
                "key_observation": "Single most important observation"
            }}
            """
            
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            result = json.loads(response.content.replace("```json", "").replace("```", ""))
            
            state.behavioral_summary = result.get("summary", "")
            state.speech_clarity = result.get("speech_clarity", "good")
            
            # Create enhanced transcript with annotations
            annotations = []
            if state.filler_words:
                annotations.append(f"[{len(state.filler_words)} types of fillers detected]")
            if len(state.pause_patterns) > 5:
                annotations.append(f"[{len(state.pause_patterns)} significant pauses]")
            
            state.enhanced_transcript = state.transcript
            if annotations:
                state.enhanced_transcript += f"\n\n--- Voice Analysis: {' '.join(annotations)} ---"
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Summary generation error: {e}")
            state.behavioral_summary = "Voice analysis complete. See metrics for details."
            return state
    
    async def process_audio(
        self,
        audio_data: bytes,
        sample_rate: int = 16000
    ) -> Dict[str, Any]:
        """Process audio and return comprehensive analysis"""
        try:
            initial_state = VoiceProcessingState(
                audio_data=audio_data,
                sample_rate=sample_rate
            )
            
            final_state = await self.workflow.ainvoke(initial_state)
            
            return {
                "success": True,
                "transcript": final_state.transcript,
                "enhanced_transcript": final_state.enhanced_transcript,
                "language": final_state.language,
                "confidence": final_state.confidence,
                "duration": final_state.audio_duration,
                "metrics": {
                    "speaking_rate": final_state.speaking_rate,
                    "pitch_variation": final_state.pitch_variation,
                    "volume_stability": final_state.volume_stability,
                    "speech_clarity": final_state.speech_clarity,
                    "engagement_score": final_state.engagement_score
                },
                "emotion": {
                    "detected": final_state.detected_emotion,
                    "confidence": final_state.emotion_confidence
                },
                "disfluencies": {
                    "filler_words": final_state.filler_words,
                    "hesitations": len(final_state.hesitations),
                    "self_corrections": final_state.self_corrections
                },
                "behavioral_signals": {
                    "confidence_indicators": final_state.confidence_indicators,
                    "stress_indicators": final_state.stress_indicators
                },
                "summary": final_state.behavioral_summary,
                "pause_patterns": final_state.pause_patterns[:5]  # Top 5 pauses
            }
            
        except Exception as e:
            logger.error(f"âŒ Voice processing error: {e}")
            return {
                "success": False,
                "error": str(e),
                "transcript": ""
            }
    
    async def process_stream_chunk(
        self,
        chunk: bytes,
        is_final: bool = False
    ) -> Optional[Dict[str, Any]]:
        """Process a streaming audio chunk"""
        try:
            # Add to buffer
            self.audio_buffer.write(chunk)
            
            # Process when we have enough data or it's the final chunk
            buffer_size = self.audio_buffer.tell()
            min_process_size = 16000 * 2 * 3  # 3 seconds of 16kHz 16-bit audio
            
            if buffer_size >= min_process_size or is_final:
                self.audio_buffer.seek(0)
                audio_data = self.audio_buffer.read()
                
                # Reset buffer
                self.audio_buffer = io.BytesIO()
                
                # Process accumulated audio
                return await self.process_audio(audio_data)
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Stream processing error: {e}")
            return None
    
    def reset_stream(self):
        """Reset the streaming buffer"""
        self.audio_buffer = io.BytesIO()


# Singleton instance
_voice_processor = None

def get_voice_processor() -> VoiceNativeProcessor:
    """Get singleton voice processor instance"""
    global _voice_processor
    if _voice_processor is None:
        _voice_processor = VoiceNativeProcessor()
    return _voice_processor
