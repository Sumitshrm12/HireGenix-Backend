"""
ðŸ§  PERSISTENT MEMORY LAYER - The Long-Term Brain of HireGenix Agentic AI
Implements cross-session conversation memory with vector embeddings for 100% context awareness.

Features:
- Persistent conversation memory per candidate-company pair
- Vector-based semantic search across all past interactions
- Skill verification progress tracking across sessions
- Multi-round interview context sharing
- Automatic memory consolidation and summarization
- Real-time memory retrieval during interviews

Tech Stack:
- LangChain Memory (ConversationSummaryBufferMemory + VectorStoreRetrieverMemory)
- Redis Vector Store for fast semantic search
- LangGraph state persistence
- Automatic memory decay and consolidation
"""

import os
import json
import asyncio
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import logging

# Redis with vector support
import redis
from redis.commands.search.query import Query

# LangChain Memory
from langchain.memory import (
    ConversationBufferMemory,
    ConversationSummaryBufferMemory,
    VectorStoreRetrieverMemory,
    CombinedMemory
)
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import Redis as RedisVectorStore
from langchain_core.prompts import PromptTemplate

# Sentence Transformers for local embeddings
from sentence_transformers import SentenceTransformer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MemoryType(str, Enum):
    """Types of memory stored"""
    CONVERSATION = "conversation"  # Full conversation history
    SKILL_VERIFICATION = "skill_verification"  # Skills verified and scores
    BEHAVIORAL_OBSERVATION = "behavioral_observation"  # Non-verbal signals observed
    INTERVIEW_SUMMARY = "interview_summary"  # Consolidated interview summaries
    CANDIDATE_INSIGHT = "candidate_insight"  # AI-generated insights about candidate
    QUESTION_ASKED = "question_asked"  # Track questions to avoid repetition
    FOLLOW_UP_NEEDED = "follow_up_needed"  # Topics needing more exploration


@dataclass
class MemoryEntry:
    """Single memory entry structure"""
    memory_id: str
    candidate_id: str
    company_id: str
    job_id: str
    session_id: str
    memory_type: MemoryType
    content: Dict[str, Any]
    embedding: List[float] = field(default_factory=list)
    importance_score: float = 0.5  # 0-1 scale
    created_at: str = ""
    accessed_count: int = 0
    last_accessed: str = ""
    expires_at: Optional[str] = None  # For time-decay memories
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "memory_id": self.memory_id,
            "candidate_id": self.candidate_id,
            "company_id": self.company_id,
            "job_id": self.job_id,
            "session_id": self.session_id,
            "memory_type": self.memory_type.value,
            "content": self.content,
            "embedding": self.embedding,
            "importance_score": self.importance_score,
            "created_at": self.created_at or datetime.utcnow().isoformat(),
            "accessed_count": self.accessed_count,
            "last_accessed": self.last_accessed or datetime.utcnow().isoformat(),
            "expires_at": self.expires_at
        }


class PersistentMemoryLayer:
    """
    Enterprise-grade persistent memory system for agentic AI interviews.
    Provides 100% context awareness across all sessions and interactions.
    """
    
    def __init__(self):
        logger.info("ðŸ§  Initializing Persistent Memory Layer...")
        
        # Redis connection
        redis_password = os.getenv("REDIS_PASSWORD", "")
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            password=redis_password if redis_password else None,
            decode_responses=True
        )
        
        # Azure OpenAI for memory synthesis
        self.llm = AzureChatOpenAI(
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"),
            temperature=0.3
        )
        
        # Embeddings
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Memory consolidation settings
        self.MAX_SHORT_TERM_MEMORIES = 50
        self.CONSOLIDATION_THRESHOLD = 20
        self.MEMORY_DECAY_DAYS = 90
        
        # Initialize indexes
        self._init_memory_indexes()
        
        logger.info("âœ… Persistent Memory Layer initialized")
    
    def _init_memory_indexes(self):
        """Initialize Redis search indexes for memory retrieval"""
        try:
            from redis.commands.search.field import TextField, VectorField, NumericField, TagField
            from redis.commands.search.indexDefinition import IndexDefinition, IndexType
            
            # Memory index schema
            memory_schema = (
                TextField("$.memory_id", as_name="memory_id"),
                TagField("$.candidate_id", as_name="candidate_id"),
                TagField("$.company_id", as_name="company_id"),
                TagField("$.job_id", as_name="job_id"),
                TagField("$.session_id", as_name="session_id"),
                TagField("$.memory_type", as_name="memory_type"),
                NumericField("$.importance_score", as_name="importance_score"),
                TextField("$.content", as_name="content"),
                VectorField(
                    "$.embedding",
                    "FLAT",
                    {
                        "TYPE": "FLOAT32",
                        "DIM": 384,
                        "DISTANCE_METRIC": "COSINE"
                    },
                    as_name="embedding"
                )
            )
            
            self.redis_client.ft("memory_index").create_index(
                memory_schema,
                definition=IndexDefinition(prefix=["memory:"], index_type=IndexType.JSON)
            )
            logger.info("âœ… Memory index created")
            
        except Exception as e:
            if "Index already exists" not in str(e):
                logger.warning(f"âš ï¸ Memory index creation: {e}")
    
    def _generate_memory_id(self, candidate_id: str, content: str) -> str:
        """Generate unique memory ID"""
        content_hash = hashlib.md5(f"{candidate_id}:{content}:{datetime.utcnow().isoformat()}".encode()).hexdigest()[:12]
        return f"mem_{candidate_id[:8]}_{content_hash}"
    
    def _generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        return self.embedder.encode(text).tolist()
    
    async def store_memory(
        self,
        candidate_id: str,
        company_id: str,
        job_id: str,
        session_id: str,
        memory_type: MemoryType,
        content: Dict[str, Any],
        importance_score: float = 0.5
    ) -> str:
        """Store a new memory entry"""
        try:
            # Generate embedding from content
            content_text = json.dumps(content) if isinstance(content, dict) else str(content)
            embedding = self._generate_embedding(content_text[:2000])
            
            memory_id = self._generate_memory_id(candidate_id, content_text)
            
            entry = MemoryEntry(
                memory_id=memory_id,
                candidate_id=candidate_id,
                company_id=company_id,
                job_id=job_id,
                session_id=session_id,
                memory_type=memory_type,
                content=content,
                embedding=embedding,
                importance_score=importance_score,
                created_at=datetime.utcnow().isoformat(),
                last_accessed=datetime.utcnow().isoformat()
            )
            
            # Store in Redis
            redis_key = f"memory:{memory_id}"
            try:
                self.redis_client.json().set(redis_key, "$", entry.to_dict())
            except (AttributeError, Exception):
                self.redis_client.set(redis_key, json.dumps(entry.to_dict()))
            
            logger.info(f"ðŸ’¾ Stored memory: {memory_type.value} for candidate {candidate_id[:8]}")
            
            # Check if consolidation needed
            await self._check_consolidation(candidate_id)
            
            return memory_id
            
        except Exception as e:
            logger.error(f"âŒ Memory storage error: {e}")
            return ""
    
    async def retrieve_relevant_memories(
        self,
        candidate_id: str,
        query: str,
        memory_types: Optional[List[MemoryType]] = None,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """Retrieve memories relevant to current context using semantic search"""
        try:
            # Fallback: simple key-based retrieval if RediSearch is not available
            # This uses pattern matching instead of vector search
            pattern = f"memory:mem_{candidate_id[:8]}_*"
            keys = list(self.redis_client.scan_iter(match=pattern))
            
            memories = []
            for key in keys[:top_k]:
                try:
                    data = self.redis_client.json().get(key)
                except:
                    data_str = self.redis_client.get(key)
                    data = json.loads(data_str) if data_str else None
                
                if data:
                    # Filter by memory type if specified
                    if memory_types:
                        if data.get("memory_type") not in [mt.value for mt in memory_types]:
                            continue
                    
                    memory_data = {
                        "memory_id": data.get("memory_id", ""),
                        "memory_type": data.get("memory_type", ""),
                        "content": data.get("content", {}),
                        "importance_score": float(data.get("importance_score", 0.5)),
                        "created_at": data.get("created_at", ""),
                        "relevance_score": 0.8  # Default relevance since we can't do vector search
                    }
                    memories.append(memory_data)
                    
                    # Update access count
                    await self._update_access(data.get("memory_id", ""))
            
            # Sort by importance
            memories.sort(key=lambda x: -x["importance_score"])
            
            logger.info(f"ðŸ” Retrieved {len(memories)} memories for candidate {candidate_id[:8]} (fallback mode)")
            return memories[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ Memory retrieval error: {e}")
            return []
    
    async def get_conversation_history(
        self,
        candidate_id: str,
        company_id: str,
        session_id: Optional[str] = None,
        include_all_sessions: bool = False
    ) -> List[Dict[str, Any]]:
        """Get conversation history for a candidate"""
        try:
            pattern = f"memory:mem_{candidate_id[:8]}_*"
            keys = list(self.redis_client.scan_iter(match=pattern))
            
            conversations = []
            for key in keys:
                try:
                    data = self.redis_client.json().get(key)
                except:
                    data_str = self.redis_client.get(key)
                    data = json.loads(data_str) if data_str else None
                
                if data and data.get("memory_type") == MemoryType.CONVERSATION.value:
                    if include_all_sessions or session_id is None or data.get("session_id") == session_id:
                        if data.get("company_id") == company_id:
                            conversations.append(data)
            
            # Sort by created_at
            conversations.sort(key=lambda x: x.get("created_at", ""))
            
            return conversations
            
        except Exception as e:
            logger.error(f"âŒ Conversation history error: {e}")
            return []
    
    async def get_skill_verification_status(
        self,
        candidate_id: str,
        job_id: str
    ) -> Dict[str, Any]:
        """Get status of skill verification across all sessions"""
        try:
            # Retrieve all skill verification memories
            memories = await self.retrieve_relevant_memories(
                candidate_id=candidate_id,
                query="skill verification assessment score",
                memory_types=[MemoryType.SKILL_VERIFICATION],
                top_k=50
            )
            
            # Aggregate skill data
            skills_status = {}
            for memory in memories:
                content = memory.get("content", {})
                if content.get("job_id") == job_id:
                    skill_name = content.get("skill_name", "")
                    if skill_name:
                        if skill_name not in skills_status:
                            skills_status[skill_name] = {
                                "verified": False,
                                "verification_count": 0,
                                "average_score": 0,
                                "scores": [],
                                "last_verified": None
                            }
                        
                        skills_status[skill_name]["verification_count"] += 1
                        skills_status[skill_name]["scores"].append(content.get("score", 0))
                        skills_status[skill_name]["last_verified"] = memory.get("created_at")
                        
                        if len(skills_status[skill_name]["scores"]) >= 2:
                            skills_status[skill_name]["verified"] = True
                        
                        skills_status[skill_name]["average_score"] = (
                            sum(skills_status[skill_name]["scores"]) / 
                            len(skills_status[skill_name]["scores"])
                        )
            
            return {
                "candidate_id": candidate_id,
                "job_id": job_id,
                "skills": skills_status,
                "total_skills_tracked": len(skills_status),
                "fully_verified": sum(1 for s in skills_status.values() if s["verified"])
            }
            
        except Exception as e:
            logger.error(f"âŒ Skill verification status error: {e}")
            return {"skills": {}}
    
    async def get_interview_context_summary(
        self,
        candidate_id: str,
        company_id: str
    ) -> Dict[str, Any]:
        """Get comprehensive context summary for current interview"""
        try:
            # Get all relevant memories
            all_memories = await self.retrieve_relevant_memories(
                candidate_id=candidate_id,
                query="interview summary skills experience behavioral observations",
                top_k=30
            )
            
            # Get previous interview summaries
            summaries = [m for m in all_memories if m["memory_type"] == MemoryType.INTERVIEW_SUMMARY.value]
            
            # Get behavioral observations
            behaviors = [m for m in all_memories if m["memory_type"] == MemoryType.BEHAVIORAL_OBSERVATION.value]
            
            # Get insights
            insights = [m for m in all_memories if m["memory_type"] == MemoryType.CANDIDATE_INSIGHT.value]
            
            # Get questions asked (to avoid repetition)
            questions = [m for m in all_memories if m["memory_type"] == MemoryType.QUESTION_ASKED.value]
            
            # Synthesize context
            context_prompt = f"""
            Synthesize the following interview data into a cohesive context summary:
            
            Previous Interview Summaries:
            {json.dumps([s["content"] for s in summaries[:5]], indent=2)}
            
            Behavioral Observations:
            {json.dumps([b["content"] for b in behaviors[:5]], indent=2)}
            
            Candidate Insights:
            {json.dumps([i["content"] for i in insights[:5]], indent=2)}
            
            Questions Already Asked:
            {json.dumps([q["content"].get("question", "") for q in questions[:10]], indent=2)}
            
            Create a JSON summary with:
            1. key_strengths: List of demonstrated strengths
            2. areas_to_explore: Topics needing more verification
            3. conversation_style: How the candidate typically communicates
            4. avoid_questions: Questions not to repeat
            5. follow_up_needed: Specific follow-ups from previous rounds
            6. overall_impression: Current assessment
            """
            
            response = await self.llm.ainvoke([HumanMessage(content=context_prompt)])
            
            try:
                summary = json.loads(response.content.replace("```json", "").replace("```", ""))
            except:
                summary = {
                    "key_strengths": [],
                    "areas_to_explore": [],
                    "conversation_style": "unknown",
                    "avoid_questions": [q["content"].get("question", "") for q in questions[:10]],
                    "follow_up_needed": [],
                    "overall_impression": "No previous context"
                }
            
            return {
                "candidate_id": candidate_id,
                "company_id": company_id,
                "previous_sessions": len(summaries),
                "context_summary": summary,
                "behavioral_patterns": [b["content"] for b in behaviors[:3]],
                "memory_strength": len(all_memories) / 30.0  # 0-1 based on memory density
            }
            
        except Exception as e:
            logger.error(f"âŒ Context summary error: {e}")
            return {
                "candidate_id": candidate_id,
                "previous_sessions": 0,
                "context_summary": {},
                "memory_strength": 0
            }
    
    async def store_interview_round_summary(
        self,
        candidate_id: str,
        company_id: str,
        job_id: str,
        session_id: str,
        round_type: str,
        summary: Dict[str, Any]
    ) -> str:
        """Store summary of completed interview round for multi-round context"""
        enriched_summary = {
            "round_type": round_type,
            "summary": summary,
            "key_points": summary.get("key_points", []),
            "skills_demonstrated": summary.get("skills_demonstrated", []),
            "concerns_raised": summary.get("concerns_raised", []),
            "follow_up_recommendations": summary.get("follow_up_recommendations", []),
            "overall_score": summary.get("overall_score", 0),
            "interviewer_notes": summary.get("notes", ""),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return await self.store_memory(
            candidate_id=candidate_id,
            company_id=company_id,
            job_id=job_id,
            session_id=session_id,
            memory_type=MemoryType.INTERVIEW_SUMMARY,
            content=enriched_summary,
            importance_score=0.9  # High importance for round summaries
        )
    
    async def _update_access(self, memory_id: str):
        """Update access count and timestamp for memory"""
        try:
            redis_key = f"memory:{memory_id}"
            try:
                data = self.redis_client.json().get(redis_key)
            except:
                data_str = self.redis_client.get(redis_key)
                data = json.loads(data_str) if data_str else None
            
            if data:
                data["accessed_count"] = data.get("accessed_count", 0) + 1
                data["last_accessed"] = datetime.utcnow().isoformat()
                
                try:
                    self.redis_client.json().set(redis_key, "$", data)
                except:
                    self.redis_client.set(redis_key, json.dumps(data))
                    
        except Exception as e:
            logger.debug(f"Access update error: {e}")
    
    async def _check_consolidation(self, candidate_id: str):
        """Check if memory consolidation is needed and perform it"""
        try:
            pattern = f"memory:mem_{candidate_id[:8]}_*"
            keys = list(self.redis_client.scan_iter(match=pattern))
            
            if len(keys) > self.MAX_SHORT_TERM_MEMORIES:
                logger.info(f"ðŸ”„ Consolidating memories for candidate {candidate_id[:8]}...")
                await self._consolidate_memories(candidate_id, keys)
                
        except Exception as e:
            logger.error(f"âŒ Consolidation check error: {e}")
    
    async def _consolidate_memories(self, candidate_id: str, keys: List[str]):
        """Consolidate old memories into summary memories"""
        try:
            # Get all memories
            memories = []
            for key in keys:
                try:
                    data = self.redis_client.json().get(key)
                except:
                    data_str = self.redis_client.get(key)
                    data = json.loads(data_str) if data_str else None
                
                if data:
                    memories.append(data)
            
            # Sort by importance and recency
            memories.sort(key=lambda x: (
                -x.get("importance_score", 0),
                -x.get("accessed_count", 0)
            ))
            
            # Keep top memories, consolidate rest
            to_keep = memories[:self.CONSOLIDATION_THRESHOLD]
            to_consolidate = memories[self.CONSOLIDATION_THRESHOLD:]
            
            if to_consolidate:
                # Consolidate into summary
                consolidation_content = {
                    "consolidated_from": len(to_consolidate),
                    "date_range": {
                        "start": min(m.get("created_at", "") for m in to_consolidate),
                        "end": max(m.get("created_at", "") for m in to_consolidate)
                    },
                    "memory_types_included": list(set(m.get("memory_type") for m in to_consolidate)),
                    "key_content": [m.get("content") for m in to_consolidate[:10]]
                }
                
                # Store consolidated memory
                await self.store_memory(
                    candidate_id=candidate_id,
                    company_id=to_consolidate[0].get("company_id", ""),
                    job_id=to_consolidate[0].get("job_id", ""),
                    session_id="consolidated",
                    memory_type=MemoryType.CANDIDATE_INSIGHT,
                    content=consolidation_content,
                    importance_score=0.7
                )
                
                # Delete consolidated memories
                for m in to_consolidate:
                    self.redis_client.delete(f"memory:{m.get('memory_id')}")
                
                logger.info(f"âœ… Consolidated {len(to_consolidate)} memories for candidate {candidate_id[:8]}")
                
        except Exception as e:
            logger.error(f"âŒ Memory consolidation error: {e}")


# Singleton instance
_memory_layer = None

def get_memory_layer() -> PersistentMemoryLayer:
    """Get singleton memory layer instance"""
    global _memory_layer
    if _memory_layer is None:
        _memory_layer = PersistentMemoryLayer()
    return _memory_layer
