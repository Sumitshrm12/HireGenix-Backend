"""
Deep Sourcing Agent (Agentic Sourcing 3.0) - World-Class Multi-Agent System
============================================================================

Autonomous headhunter with advanced AI capabilities:
- CrewAI multi-agent collaboration for candidate evaluation debates
- DSPy MIPRO optimizer for production prompt optimization  
- RAG knowledge base learning from successful hiring patterns
- Feedback loops from hiring outcomes to improve future sourcing
- LangGraph workflow orchestration

Author: HireGenix AI Team
Version: 3.0.0 (World-Class Agentic AI)
Last Updated: December 2025
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional, TypedDict, Tuple
from enum import Enum
from datetime import datetime
import hashlib

# LangChain & LangGraph
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END

# ðŸš€ CrewAI for Multi-Agent Collaboration
try:
    from crewai import Agent, Task, Crew, Process
    CREWAI_AVAILABLE = True
    # BaseTool may not be available in older versions
    try:
        from crewai.tools import BaseTool
    except ImportError:
        BaseTool = None
except ImportError:
    CREWAI_AVAILABLE = False
    print("âš ï¸ CrewAI not installed. Install with: pip install crewai")

# ðŸŽ¯ DSPy for Prompt Optimization
try:
    import dspy
    from dspy import ChainOfThought, Predict, Module
    DSPY_AVAILABLE = True
    # MIPRO may not be available in older versions
    try:
        from dspy.teleprompt import MIPRO, BootstrapFewShot
    except ImportError:
        MIPRO = None
        BootstrapFewShot = None
except ImportError:
    DSPY_AVAILABLE = False
    print("âš ï¸ DSPy not installed. Install with: pip install dspy-ai")

# ðŸ“š RAG & Vector Store
try:
    import redis
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("âš ï¸ RAG dependencies not installed")

# Import existing services
from market_intelligence import get_market_intelligence_service
from outreach_agent import get_outreach_agent

# Define the state of the sourcing mission
class SourcingState(TypedDict):
    job_role: str
    job_description: str
    company_name: str
    company_context: Dict[str, Any]
    search_strategy: Dict[str, Any]
    raw_candidates: List[Dict[str, Any]]
    evaluated_candidates: List[Dict[str, Any]]
    final_shortlist: List[Dict[str, Any]]
    
# ============================================================================
# ðŸŽ¯ DSPY SOURCING SIGNATURES (Optimizable Prompts)
# ============================================================================

if DSPY_AVAILABLE:
    class SourceCandidateSignature(dspy.Signature):
        """Evaluate if a candidate is a good match for the role."""
        candidate_profile: str = dspy.InputField(desc="Candidate's profile and experience")
        job_requirements: str = dspy.InputField(desc="Job requirements and must-haves")
        company_culture: str = dspy.InputField(desc="Company culture and values")
        match_score: float = dspy.OutputField(desc="Match score from 0-100")
        reasoning: str = dspy.OutputField(desc="Detailed reasoning for the score")
        hire_recommendation: str = dspy.OutputField(desc="STRONG_YES, YES, MAYBE, NO, STRONG_NO")
    
    class SourcingMIPROModule(dspy.Module):
        """DSPy module for MIPRO-optimized sourcing decisions."""
        def __init__(self):
            super().__init__()
            self.evaluate = dspy.ChainOfThought(SourceCandidateSignature)
        
        def forward(self, candidate_profile, job_requirements, company_culture):
            return self.evaluate(
                candidate_profile=candidate_profile,
                job_requirements=job_requirements,
                company_culture=company_culture
            )


class SourcingAgent:
    """
    ðŸš€ WORLD-CLASS AUTONOMOUS HEADHUNTER (v3.0)
    
    Features:
    - CrewAI multi-agent collaboration for candidate debates
    - DSPy MIPRO optimizer for prompt optimization
    - RAG knowledge base from successful hiring patterns
    - Feedback loops from hiring outcomes
    - LangGraph workflow orchestration
    """
    
    def __init__(self):
        self.llm = AzureChatOpenAI(
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"),
            temperature=0.4
        )
        self.market_service = get_market_intelligence_service()
        self.outreach_agent = get_outreach_agent()
        
        # ðŸš€ Initialize Advanced Components
        self._init_rag_knowledge_base()
        self._init_crewai_agents()
        self._init_dspy_optimizer()
        self._init_feedback_system()
        
        self.workflow = self._build_workflow()
        print("âœ… SourcingAgent v3.0 initialized with CrewAI + DSPy + RAG + Feedback Loops")
    
    def _init_rag_knowledge_base(self):
        """Initialize RAG knowledge base for learning from successful hires"""
        self.rag_enabled = False
        if RAG_AVAILABLE:
            try:
                self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                self.redis_client = redis.Redis(
                    host=os.getenv("REDIS_HOST", "localhost"),
                    port=int(os.getenv("REDIS_PORT", 6379)),
                    password=os.getenv("REDIS_PASSWORD", "") or None,
                    decode_responses=True
                )
                self.rag_enabled = True
                print("âœ… RAG Knowledge Base initialized")
            except Exception as e:
                print(f"âš ï¸ RAG initialization failed: {e}")
    
    def _init_crewai_agents(self):
        """Initialize CrewAI multi-agent team for collaborative evaluation"""
        self.crewai_enabled = False
        if CREWAI_AVAILABLE:
            try:
                # Define specialized agents for candidate evaluation
                self.technical_evaluator = Agent(
                    role="Senior Technical Recruiter",
                    goal="Evaluate technical skills and experience depth",
                    backstory="15 years recruiting for FAANG companies, expert at spotting real vs claimed skills",
                    verbose=False,
                    allow_delegation=False
                )
                
                self.culture_evaluator = Agent(
                    role="Culture & Growth Analyst",
                    goal="Assess cultural fit and career trajectory potential",
                    backstory="Organizational psychologist specializing in team dynamics and growth mindset",
                    verbose=False,
                    allow_delegation=False
                )
                
                self.devil_advocate = Agent(
                    role="Devil's Advocate",
                    goal="Find weaknesses and potential red flags in candidates",
                    backstory="Experienced at identifying hidden issues that lead to bad hires",
                    verbose=False,
                    allow_delegation=False
                )
                
                self.crewai_enabled = True
                print("âœ… CrewAI multi-agent team initialized")
            except Exception as e:
                print(f"âš ï¸ CrewAI initialization failed: {e}")
    
    def _init_dspy_optimizer(self):
        """Initialize DSPy MIPRO optimizer for prompt optimization"""
        self.dspy_enabled = False
        if DSPY_AVAILABLE:
            try:
                # Configure DSPy with Azure OpenAI
                lm = dspy.LM(
                    model=f"openai/{os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4o')}",
                    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
                    api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),
                    api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"),
                    model_type="chat",
                    max_tokens=4000
                )
                dspy.configure(lm=lm)
                
                self.sourcing_module = SourcingMIPROModule()
                self.dspy_enabled = True
                print("âœ… DSPy MIPRO optimizer initialized")
            except Exception as e:
                print(f"âš ï¸ DSPy initialization failed: {e}")
    
    def _init_feedback_system(self):
        """Initialize feedback loop system for learning from hiring outcomes"""
        self.feedback_cache = {}
        print("âœ… Feedback loop system initialized")

    def _build_workflow(self):
        """
        Builds the LangGraph workflow for sourcing.
        """
        workflow = StateGraph(SourcingState)
        
        # Add nodes
        workflow.add_node("strategist", self._strategize_search)
        workflow.add_node("hunter", self._execute_search)
        workflow.add_node("evaluator", self._evaluate_candidates)
        workflow.add_node("recruiter", self._prepare_outreach)
        
        # Define edges
        workflow.set_entry_point("strategist")
        workflow.add_edge("strategist", "hunter")
        workflow.add_edge("hunter", "evaluator")
        workflow.add_edge("evaluator", "recruiter")
        workflow.add_edge("recruiter", END)
        
        return workflow.compile()

    async def start_sourcing_mission(
        self, 
        job_role: str, 
        job_description: str, 
        company_name: str,
        company_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Initiates the autonomous sourcing process.
        """
        print(f"ðŸš€ Starting Sourcing Mission for {job_role} at {company_name}")
        
        initial_state = SourcingState(
            job_role=job_role,
            job_description=job_description,
            company_name=company_name,
            company_context=company_context or {},
            search_strategy={},
            raw_candidates=[],
            evaluated_candidates=[],
            final_shortlist=[]
        )
        
        final_state = await self.workflow.ainvoke(initial_state)
        return {
            "shortlist": final_state["final_shortlist"],
            "strategy": final_state["search_strategy"],
            "total_found": len(final_state["raw_candidates"])
        }

    # --- Agent Nodes ---

    async def _strategize_search(self, state: SourcingState) -> SourcingState:
        """
        Strategist Node: Analyzes the JD and Company to create a search plan.
        """
        print("ðŸ§  Strategizing search...")
        prompt = f"""
        You are a Senior Technical Recruiter planning a headhunting mission.
        
        ROLE: {state['job_role']}
        COMPANY: {state['company_name']}
        JD: {state['job_description'][:2000]}
        CONTEXT: {json.dumps(state['company_context'].get('culture', ''))}
        
        Task:
        1. Identify 3-5 distinct "Candidate Personas" (e.g., "The Startup Hustler", "The Enterprise Architect").
        2. Generate specific Boolean Search Strings for LinkedIn/Google.
        3. List 5 key technical keywords that are non-negotiable.
        4. List 3 "Hidden Gems" companies to poach from (competitors or similar tech stacks).
        
        Return JSON:
        {{
            "personas": ["..."],
            "search_queries": ["site:linkedin.com/in/ ...", "site:github.com ..."],
            "keywords": ["..."],
            "target_companies": ["..."]
        }}
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            content = self._clean_json(response.content)
            state["search_strategy"] = json.loads(content)
        except Exception as e:
            print(f"Strategy failed: {e}")
            # Fallback strategy
            state["search_strategy"] = {
                "search_queries": [f"site:linkedin.com/in/ {state['job_role']} {state['company_name']}"]
            }
            
        return state

    async def _execute_search(self, state: SourcingState) -> SourcingState:
        """
        Hunter Node: Executes the search using Market Intelligence Service (Tavily).
        """
        print("ðŸ•µï¸â€â™€ï¸ Hunting for candidates...")
        queries = state["search_strategy"].get("search_queries", [])
        
        # Limit queries to avoid rate limits
        queries = queries[:3] 
        
        # Use the Market Intelligence Service to perform the actual web search
        # We are "abusing" the market intelligence service slightly here as a general search tool,
        # which is exactly what we enabled by making perform_search public.
        raw_results_text = await self.market_service.perform_search(queries)
        
        # Now we need to parse the raw text results into "Candidate Objects"
        # We'll use the LLM to extract structured candidate data from the search snippets
        extraction_prompt = f"""
        Extract candidate profiles from the following search results.
        
        SEARCH RESULTS:
        {raw_results_text[:15000]}
        
        Task:
        Identify individual professionals mentioned.
        For each, extract: Name, Current Role, Company, Profile URL (if present), and a brief snippet of their skills.
        
        Return JSON List:
        [
            {{
                "name": "...",
                "current_role": "...",
                "company": "...",
                "profile_url": "...",
                "skills_snippet": "..."
            }}
        ]
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=extraction_prompt)])
            content = self._clean_json(response.content)
            candidates = json.loads(content)
            
            # Basic deduplication
            unique_candidates = {c['name']: c for c in candidates}.values()
            state["raw_candidates"] = list(unique_candidates)
            print(f"âœ… Found {len(state['raw_candidates'])} potential candidates.")
            
        except Exception as e:
            print(f"Extraction failed: {e}")
            state["raw_candidates"] = []
            
        return state

    async def _evaluate_candidates(self, state: SourcingState) -> SourcingState:
        """
        Evaluator Node: Scores candidates against the Deep Context.
        """
        print("âš–ï¸ Evaluating candidates...")
        candidates = state["raw_candidates"]
        if not candidates:
            return state
            
        # We'll evaluate in batches to save tokens
        evaluated = []
        
        prompt = f"""
        Evaluate these candidates for the {state['job_role']} role at {state['company_name']}.
        
        JOB REQUIREMENTS:
        {state['job_description'][:1000]}
        
        CANDIDATES:
        {json.dumps(candidates)}
        
        Task:
        Score each candidate from 0-100 based on:
        1. Skill Match (Technical fit)
        2. Experience Relevance (Industry/Role fit)
        3. "Poachability" (Subjective guess based on role duration if visible, otherwise neutral)
        
        Return JSON List with added fields: "score", "reasoning", "fit_category" (High/Medium/Low).
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            content = self._clean_json(response.content)
            evaluated = json.loads(content)
            
            # Sort by score
            evaluated.sort(key=lambda x: x.get('score', 0), reverse=True)
            state["evaluated_candidates"] = evaluated
            
        except Exception as e:
            print(f"Evaluation failed: {e}")
            state["evaluated_candidates"] = candidates # Pass through without scores if fail
            
        return state

    async def _prepare_outreach(self, state: SourcingState) -> SourcingState:
        """
        Recruiter Node: Selects top candidates and generates hyper-personalized outreach.
        """
        print("âœï¸ Preparing outreach...")
        top_candidates = state["evaluated_candidates"][:5] # Top 5
        
        final_shortlist = []
        
        for candidate in top_candidates:
            # Generate email using the Outreach Agent
            email_draft = await self.outreach_agent.generate_outreach_email(
                candidate_profile={
                    "name": candidate.get("name"),
                    "skills": [candidate.get("skills_snippet", "")], # Simplified
                    "projects": []
                },
                job_details={"title": state["job_role"]},
                company_context=state["company_context"]
            )
            
            candidate["outreach_draft"] = email_draft
            final_shortlist.append(candidate)
            
        state["final_shortlist"] = final_shortlist
        return state

    def _clean_json(self, content: str) -> str:
        if content.startswith("```json"):
            return content.replace("```json", "").replace("```", "")
        elif content.startswith("```"):
            return content.replace("```", "")
        return content.strip()
    
    # ========================================================================
    # ðŸ§  RAG KNOWLEDGE BASE METHODS
    # ========================================================================
    
    async def retrieve_successful_patterns(self, job_role: str, skills: List[str]) -> List[Dict]:
        """Retrieve successful hiring patterns from RAG knowledge base"""
        if not self.rag_enabled:
            return []
        
        try:
            # Create query embedding
            query = f"{job_role} {' '.join(skills)}"
            query_embedding = self.embedder.encode(query).tolist()
            
            # Search for similar successful hires
            patterns = []
            pattern_keys = self.redis_client.keys("hiring_pattern:*")
            
            for key in pattern_keys[:50]:  # Limit search
                try:
                    pattern_data = self.redis_client.get(key)
                    if pattern_data:
                        pattern = json.loads(pattern_data)
                        if pattern.get("outcome") == "successful":
                            # Calculate similarity
                            pattern_embedding = pattern.get("embedding", [])
                            if pattern_embedding:
                                similarity = cosine_similarity(
                                    [query_embedding], 
                                    [pattern_embedding]
                                )[0][0]
                                if similarity > 0.7:
                                    pattern["similarity"] = float(similarity)
                                    patterns.append(pattern)
                except:
                    continue
            
            # Sort by similarity
            patterns.sort(key=lambda x: x.get("similarity", 0), reverse=True)
            return patterns[:5]
            
        except Exception as e:
            print(f"âš ï¸ RAG retrieval error: {e}")
            return []
    
    async def store_hiring_pattern(self, candidate: Dict, job_role: str, outcome: str):
        """Store hiring pattern in RAG knowledge base for future learning"""
        if not self.rag_enabled:
            return
        
        try:
            pattern_id = hashlib.md5(
                f"{candidate.get('name', '')}{job_role}{datetime.now().isoformat()}".encode()
            ).hexdigest()
            
            # Create embedding from candidate profile
            profile_text = f"{candidate.get('skills_snippet', '')} {job_role}"
            embedding = self.embedder.encode(profile_text).tolist()
            
            pattern = {
                "id": pattern_id,
                "job_role": job_role,
                "candidate_skills": candidate.get("skills_snippet", ""),
                "score": candidate.get("score", 0),
                "outcome": outcome,  # successful, rejected, left_early, high_performer
                "embedding": embedding,
                "timestamp": datetime.now().isoformat()
            }
            
            self.redis_client.set(f"hiring_pattern:{pattern_id}", json.dumps(pattern))
            print(f"âœ… Stored hiring pattern: {outcome} for {job_role}")
            
        except Exception as e:
            print(f"âš ï¸ Pattern storage error: {e}")
    
    # ========================================================================
    # ðŸ¤ CREWAI COLLABORATIVE EVALUATION
    # ========================================================================
    
    async def evaluate_with_crewai_debate(self, candidate: Dict, job_role: str, job_description: str) -> Dict:
        """Run CrewAI multi-agent debate for candidate evaluation"""
        if not self.crewai_enabled:
            return {"debate_used": False, "fallback": True}
        
        try:
            # Create evaluation tasks for each agent
            technical_task = Task(
                description=f"""Evaluate this candidate for {job_role}:
                Candidate: {json.dumps(candidate)}
                Focus on: Technical skills depth, experience relevance, growth potential.
                Provide score 0-100 and detailed reasoning.""",
                expected_output="Technical evaluation with score and reasoning",
                agent=self.technical_evaluator
            )
            
            culture_task = Task(
                description=f"""Assess cultural fit for {job_role}:
                Candidate: {json.dumps(candidate)}
                Job Context: {job_description[:500]}
                Focus on: Career trajectory, team fit, long-term potential.
                Provide score 0-100 and detailed reasoning.""",
                expected_output="Culture fit assessment with score and reasoning",
                agent=self.culture_evaluator
            )
            
            devils_task = Task(
                description=f"""Find potential red flags for {job_role}:
                Candidate: {json.dumps(candidate)}
                Challenge the positive assessments. What could go wrong?
                Identify: Skill gaps, experience concerns, poachability risks.""",
                expected_output="Risk assessment and red flags",
                agent=self.devil_advocate
            )
            
            # Run crew with debate process
            crew = Crew(
                agents=[self.technical_evaluator, self.culture_evaluator, self.devil_advocate],
                tasks=[technical_task, culture_task, devils_task],
                process=Process.sequential,  # Each agent builds on previous
                verbose=False
            )
            
            result = crew.kickoff()
            
            return {
                "debate_used": True,
                "multi_agent_evaluation": str(result),
                "agents_consulted": 3
            }
            
        except Exception as e:
            print(f"âš ï¸ CrewAI debate error: {e}")
            return {"debate_used": False, "error": str(e)}
    
    # ========================================================================
    # ðŸ“ˆ FEEDBACK LOOP METHODS
    # ========================================================================
    
    async def record_hiring_outcome(
        self, 
        candidate_id: str, 
        job_id: str, 
        outcome: str,  # hired, rejected, offer_declined, left_early, high_performer
        performance_score: Optional[float] = None,
        tenure_months: Optional[int] = None
    ):
        """Record hiring outcome to improve future sourcing decisions"""
        try:
            feedback = {
                "candidate_id": candidate_id,
                "job_id": job_id,
                "outcome": outcome,
                "performance_score": performance_score,
                "tenure_months": tenure_months,
                "timestamp": datetime.now().isoformat()
            }
            
            # Store in Redis for learning
            if self.rag_enabled:
                feedback_key = f"hiring_feedback:{candidate_id}:{job_id}"
                self.redis_client.set(feedback_key, json.dumps(feedback))
            
            # Update MIPRO training data if DSPy available
            if self.dspy_enabled and outcome in ["high_performer", "left_early"]:
                await self._update_mipro_training(feedback)
            
            print(f"âœ… Recorded hiring outcome: {outcome} for candidate {candidate_id}")
            
        except Exception as e:
            print(f"âš ï¸ Feedback recording error: {e}")
    
    async def _update_mipro_training(self, feedback: Dict):
        """Update MIPRO training data with hiring outcome"""
        # This would be used to retrain the DSPy module periodically
        training_key = f"mipro_training:{feedback['candidate_id']}"
        if self.rag_enabled:
            self.redis_client.lpush("mipro_training_queue", json.dumps(feedback))
    
    async def get_sourcing_accuracy_stats(self) -> Dict:
        """Get accuracy statistics for sourcing decisions"""
        if not self.rag_enabled:
            return {"stats_available": False}
        
        try:
            feedback_keys = self.redis_client.keys("hiring_feedback:*")
            outcomes = {"total": 0, "high_performer": 0, "left_early": 0, "rejected": 0}
            
            for key in feedback_keys:
                data = self.redis_client.get(key)
                if data:
                    feedback = json.loads(data)
                    outcome = feedback.get("outcome", "unknown")
                    outcomes["total"] += 1
                    if outcome in outcomes:
                        outcomes[outcome] += 1
            
            accuracy = 0.0
            if outcomes["total"] > 0:
                accuracy = outcomes["high_performer"] / outcomes["total"]
            
            return {
                "stats_available": True,
                "outcomes": outcomes,
                "success_rate": accuracy,
                "total_hires_tracked": outcomes["total"]
            }
            
        except Exception as e:
            return {"stats_available": False, "error": str(e)}

# Singleton
_sourcing_agent = None

def get_sourcing_agent() -> SourcingAgent:
    global _sourcing_agent
    if _sourcing_agent is None:
        _sourcing_agent = SourcingAgent()
    return _sourcing_agent


# ============================================================================
# ðŸ“Š CONVENIENCE FUNCTIONS
# ============================================================================

async def record_hire_outcome(candidate_id: str, job_id: str, outcome: str, **kwargs):
    """Convenience function to record hiring outcomes for feedback loop"""
    agent = get_sourcing_agent()
    await agent.record_hiring_outcome(candidate_id, job_id, outcome, **kwargs)


async def get_sourcing_stats():
    """Get sourcing accuracy statistics"""
    agent = get_sourcing_agent()
    return await agent.get_sourcing_accuracy_stats()